# -*- coding: utf-8 -*-
"""Dicoding ML-NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c2rLsb5rlj6wsymdxvJKGlWvKHhXTiDq
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import Sequential

from keras.preprocessing import text,sequence
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

!pip install -q kaggle

from google.colab import files
files.upload()

# membuat directory dan mengubah izin file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset

# unzip
!mkdir fake-and-real-news-dataset
!unzip fake-and-real-news-dataset.zip -d fake-and-real-news-dataset
!ls fake-and-real-news-dataset

"""Load Dataset"""

true_data = pd.read_csv('/content/fake-and-real-news-dataset/True.csv')
fake_data = pd.read_csv('/content/fake-and-real-news-dataset/Fake.csv')

true_data.head()

fake_data.head()

true_data.info()

fake_data.info()

true_data.subject.value_counts()

fake_data.subject.value_counts()

#add column
true_data['label'] = 1
fake_data['label'] = 0

true_data.head()

news = pd.concat([true_data, fake_data], ignore_index=True, sort=False)
news.head()

news.tail()

news.isnull().sum()

counts = news['label'].value_counts()
counts.index = ['Real News' if idx == 1 else 'Fake News' for idx in counts.index]

plt.figure(figsize=(4, 3))
plt.bar(counts.index, counts.values, color=['red', 'green'])
plt.title('Count of Real News and Fake News')
plt.xlabel(None)
plt.ylabel(None)
plt.show()

news.subject.value_counts()
plt.figure(figsize=(10, 5))
ax = sns.countplot(x="subject",  hue='label', data=news)
plt.title("Subject Distribution of News: Fake vs Real")

"""Data Cleaning"""

news['text']= news['subject'] + " " + news['title'] + " " + news['text']

news.head()

del news['title']
del news['subject']
del news['date']
news.head()

#Remove HTML content
!pip install bs4

clear_text = news.text[10]
clear_text

from bs4 import BeautifulSoup

soup = BeautifulSoup(clear_text, "html.parser")
clear_text = soup.get_text()
clear_text

import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
import string
import re

stwd = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stwd.update(punctuation)

def remove_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def remove_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

def remove_characters(text):
    return re.sub("[^a-zA-Z]"," ",text)

def remove_stopwords(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stwd:
            final_text.append(i.strip())
    return " ".join(final_text)

def final_clean(text):
    text = remove_html(text)
    text = remove_brackets(text)
    text = remove_stopwords(text)
    return text

#Apply function on review column
news['text'] = news['text'].apply(final_clean)

news.head()

from wordcloud import WordCloud,STOPWORDS

plt.figure(figsize = (8,8))
wc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(" ".join(news[news.label == 0].text))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (8,8))
wc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(" ".join(news[news.label == 1].text))
plt.imshow(wc , interpolation = 'bilinear')

"""Modeling"""

x_train, x_test, y_train, y_test = train_test_split(news['text'], news['label'], test_size=0.2, random_state=0)

vocab_size = 10000
maxlen = 300
oov_tok = "<OOV>"

tokenizer = text.Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(x_train)
tokenized_train = tokenizer.texts_to_sequences(x_train)
x_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)

tokenized_test = tokenizer.texts_to_sequences(x_test)
x_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)

batch_size = 256
embed_size = 100

# model
model = Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=maxlen, trainable=False),
    tf.keras.layers.LSTM(128, return_sequences=True, recurrent_dropout=0.25, dropout=0.25),
    tf.keras.layers.LSTM(64, recurrent_dropout=0.1, dropout=0.1),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.93 and logs.get('val_accuracy')>0.93):
      self.model.stop_training = True
      print("\n The accuracy of the training set and the validation set has been achieved > 93%!")
callbacks = myCallback()

history = model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=batch_size, shuffle=True, verbose = 1, callbacks=[callbacks])

#plot akurasi
plt.figure()
plt.plot(history.history['accuracy'], label = 'Train')
plt.plot(history.history['val_accuracy'], label = 'Test')
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend()
plt.show()

#plot loss
plt.figure()
plt.plot(history.history["loss"], label = "Train")
plt.plot(history.history["val_loss"], label = "Test")
plt.title("Loss")
plt.ylabel("Loss")
plt.xlabel("epochs")
plt.legend()
plt.show()