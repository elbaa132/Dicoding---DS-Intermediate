# -*- coding: utf-8 -*-
"""Dicoding_ML_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gMEXiSwOOcaKr6ux-s59as-B-RfpaJqy

Download Dataset
"""

!pip install -q kaggle

from google.colab import files
files.upload()

# membuat directory dan mengubah izin file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d uciml/electric-power-consumption-data-set

# unzip
!mkdir electric-power-consumption-data-set
!unzip electric-power-consumption-data-set.zip -d electric-power-consumption-data-set
!ls electric-power-consumption-data-set

"""Library"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

import tensorflow as tf
from keras.models import Sequential
from keras.callbacks import EarlyStopping, ModelCheckpoint

"""Load Dataset"""

df = pd.read_csv('/content/electric-power-consumption-data-set/household_power_consumption.txt',
                 sep=';', parse_dates={'datetime' : ['Date', 'Time']}, infer_datetime_format=True,
                 low_memory=False, na_values=['nan','?'], index_col='datetime')

df.head()

"""Data Cleaning"""

df.info()

df.isnull().sum()

#fill missing value
for j in range(0,7):
        df.iloc[:,j]=df.iloc[:,j].fillna(df.iloc[:,j].mean())

df.isnull().sum()

df.head()

#jumlah baris
len(df)

hourly_df = df.resample('H').sum()

"""EDA"""

df.Global_active_power.resample('D').sum().plot(title='Global_active_power resampled over day for sum')
plt.tight_layout()
plt.show()

r = df.Global_intensity.resample('D').agg(['mean', 'std'])
r.plot(subplots = True, title='Global_intensity resampled over day')
plt.show()

# Heatmap correlations among columns of df
plt.matshow(df.corr(method='spearman'),vmax=1,vmin=-1,cmap='magma')
plt.title('without resampling', size=15)
plt.colorbar()
plt.show()

len(hourly_df)

hourly_df.head()

"""Split Data"""

x = df.iloc[:,:].values
y = df.iloc[:,0].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

scaler = MinMaxScaler(feature_range=(0,1))
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

x_train.shape

sequence_length = 30

def create_sequences(x_train, sequence_length):
    sequences = []
    targets = []
    for i in range(0,len(x_train)-sequence_length):
        sequences.append(x_train[i:i+sequence_length,:])
        targets.append(x_train[i+sequence_length,0])
    return np.array(sequences), np.array(targets)

sequences, targets = create_sequences(x_train, sequence_length)

"""Modeling"""

# model
model = Sequential([
    tf.keras.layers.LSTM(units=100, return_sequences=True, input_shape=(sequences.shape[1], 7)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(units=100, return_sequences=True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(units=100),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(units=1)
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1.0000e-04), loss='mae', metrics=['mae'])

history = model.fit(
    sequences, targets,
    validation_split = 0.2,
    batch_size = 64,
    epochs = 3,
    shuffle=False
)

threshold_mae = (df['Global_active_power'].max() - df['Global_active_power'].min()) * 10/100
threshold_mae

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

dataset_total = np.concatenate((x_train, x_test), axis=0)
inputs = dataset_total[len(dataset_total) - len(x_test) - 30:]

inputs = inputs.reshape(-1,7)
inputs = scaler.transform(inputs)

x_test_new = []
for i in range(30, len(inputs) ):
    x_test_new.append(inputs[i-30:i,:])

x_test_new = np.array(x_test_new)

predicted = model.predict(x_test_new)

replicated_array = np.repeat(predicted, 7, axis=1)
pred = scaler.inverse_transform(replicated_array)
final_predicted_values = pred[:, 0]
final_predicted_values= final_predicted_values.reshape(-1,1)
final_predicted_values.shape

from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, final_predicted_values)
mae

aa=[x for x in range(100)]
plt.figure(figsize=(8, 4))
plt.plot(aa,y_test[:100], label='Actual Global Active Power', marker='o', linestyle='-', color='b')
plt.plot(aa,final_predicted_values[:100], label='Predicted Global Active Power', linestyle='--', color='r')

plt.xlabel('Time Step')
plt.ylabel('Global Active Power')
plt.title('Actual vs. Predicted Global Active Power')
plt.legend()
plt.show()